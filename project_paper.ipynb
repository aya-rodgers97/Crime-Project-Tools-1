{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big City Crime Analysis\n",
    "\n",
    "by: Cameron Sudderth, Ayana Rodgers, Moni Buddha\n",
    "\n",
    "https://github.com/aya-rodgers97/Crime-Project-Tools-1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2024 is a Presidential Election year and crime is always a major topic with which Americans vote, both on a Federal and Local level​\n",
    "\n",
    "Every major US city publishes data related to its crimes, so we wanted to look at the major metro areas in the US to see what similarities exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API Connections​:\n",
    "\n",
    "NYC Crime Data​\n",
    "City of LA Crime Data​\n",
    "City of Chicago Crime Data​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorize the Crime Descriptions into commonly  themed categories​\n",
    "\n",
    "Identify the most significant crimes in each city​\n",
    "\n",
    "Identify the most affected gender in each city.​\n",
    "\n",
    "To find patterns in each season, day of the week and hour of the day in each city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Literature\n",
    "\n",
    "Los Angeles Neighborhood Analysis by Chaitany Krishna Kasaraneni\n",
    "Towards Data Science\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NYC API Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sodapy import Socrata\n",
    "\n",
    "# Number of Records limit\n",
    "nyc_limit = 500\n",
    "la_limit = 500\n",
    "chi_limit = 500\n",
    "\n",
    "# Connect to NYC API\n",
    "NYCAppToken = '3u5hcZ6WwKere5Mb5nm5S9mT2'\n",
    "nyc_client = Socrata(\"data.cityofnewyork.us\",\n",
    "                 NYCAppToken,\n",
    "                 username=\"Cameron.Suddreth@du.edu\",\n",
    "                 password=\"COMP4447groupproject\")\n",
    "\n",
    "# First 2000 results, returned as JSON from API / converted to Python list of\n",
    "# dictionaries by sodapy.\n",
    "nyc_results = nyc_client.get(\"5uac-w243\", limit=nyc_limit)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "nyc_df = pd.DataFrame.from_records(nyc_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LA API Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example authenticated client (needed for non-public datasets):\n",
    "LA_AppToken = 'mEU8HkgWCvfkWLHKGxfiUFecc'\n",
    "la_client = Socrata(\"data.lacity.org\",\n",
    "                 LA_AppToken,\n",
    "                 username=\"Cameron.Suddreth@du.edu\",\n",
    "                 password=\"COMP4447groupproject\")\n",
    "\n",
    "\n",
    "la_results = la_client.get(\"2nrs-mtv8\", limit=la_limit)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "la_df = pd.DataFrame.from_records(la_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chicago API Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHI_AppToken = '6rxQVr5BfXAbUUccKTodxYVdj'\n",
    "chi_client = Socrata(\"data.cityofchicago.org\",\n",
    "                    CHI_AppToken,\n",
    "                    username=\"Cameron.Suddreth@du.edu\",\n",
    "                    password=\"COMP4447groupproject\")\n",
    "chi_results = chi_client.get(\"9hwr-2zxp\", limit=chi_limit)\n",
    "chi_df = pd.DataFrame.from_records(chi_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we have established three different API connections. We have established an API connection with each of the three largest cities in the United States: New York, NY; Los Angeles, CA; and Chicago, IL. We are also able to change the number of records that we are pulling in for analysis as each of the databases combined would result in close to one million records!\n",
    "\n",
    "With each connection, we have created a separate dataframe which allowed us to easily pull in all the records from each city's database. As we now have each of the cities with their own dataframe, we will begin to merge the dataframes together to allow us to look at the data amongst the cities together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell compares the number of columns within each dataframe. This was the initial step for us to begin merging the dataset together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NYC Columns: 40\n",
      "Number of LA Columns: 26\n",
      "Number of CHI Columns: 22\n"
     ]
    }
   ],
   "source": [
    "nyc_columns = nyc_df.columns\n",
    "la_columns = la_df.columns\n",
    "chi_columns = chi_df.columns\n",
    "\n",
    "print(f'Number of NYC Columns: {len(nyc_columns)}')\n",
    "print(f'Number of LA Columns: {len(la_columns)}')\n",
    "print(f'Number of CHI Columns: {len(chi_columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reviewed the column names within the API documentation and determined what information we wanted from each city and then reduced the size of each dataframe to contain only those columns which we wanted to analyze. We then renamed each column so to allow for easier analysis and merging.\n",
    "\n",
    "Before we create the new column headers, we must also resolve any NAN values else we will receive an error. We have imported the Numpy module to fill in the Weapon user for each NYC and Chicago; the time of the crime in Chicago; and the victim sex for Chicago as this data was unavailable in each respective city's database.\n",
    "\n",
    "We have also created a new column in each city's original dataframe to help identify which city each record belongs to after we have merged the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "nyc_df['Weapon'] = np.nan\n",
    "chi_df['Weapon'] = np.nan\n",
    "chi_df['Time'] = np.nan\n",
    "chi_df['Victim Sex'] = np.nan\n",
    "nyc_df['City'] = 'NYC'\n",
    "la_df['City'] = \"LA\"\n",
    "chi_df['City'] = 'CHI'\n",
    "\n",
    "la_df = la_df[['dr_no', 'date_rptd', 'date_occ', 'crm_cd', 'crm_cd_desc', \n",
    "'weapon_desc', 'vict_sex', 'lat', 'lon', 'City']]\n",
    "nyc_df = nyc_df[['cmplnt_num', 'cmplnt_fr_dt', 'cmplnt_fr_tm','ky_cd', 'ofns_desc', \n",
    "'Weapon', 'vic_sex', 'latitude','longitude', 'City']]\n",
    "chi_df = chi_df[['id', 'date','Time', 'iucr','description','Weapon', \n",
    "'Victim Sex','latitude','longitude', 'City']]\n",
    "generic_columns = ['Case Number', 'Date', 'Time', 'Crime Code', 'Crime Description','Weapon', \n",
    "'Victim Sex', 'Latitude', 'Longitude', 'City']\n",
    "\n",
    "# Rename the columns\n",
    "la_df.columns = generic_columns\n",
    "nyc_df.columns = generic_columns\n",
    "chi_df.columns = generic_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By renaming the columns of the dataframe above, it simplified the merging process, so we did not have to specify what column in each dataframe to merge based on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n"
     ]
    }
   ],
   "source": [
    "combined_df = pd.concat([la_df, nyc_df, chi_df],ignore_index=True)\n",
    "print(len(combined_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "628                                         HARRASSMENT 2\n",
       "486                             OTHER MISCELLANEOUS CRIME\n",
       "1155                                            OVER $500\n",
       "1220                                               SIMPLE\n",
       "1495                              DOMESTIC BATTERY SIMPLE\n",
       "215                                     THEFT OF IDENTITY\n",
       "359                                     THEFT OF IDENTITY\n",
       "792                                        FELONY ASSAULT\n",
       "592                                         HARRASSMENT 2\n",
       "1216                                           TO VEHICLE\n",
       "293                                       ORAL COPULATION\n",
       "739                                         PETIT LARCENY\n",
       "743                          ASSAULT 3 & RELATED OFFENSES\n",
       "1111                                            OVER $500\n",
       "278                                     THEFT OF IDENTITY\n",
       "220                             OTHER MISCELLANEOUS CRIME\n",
       "196                                     THEFT OF IDENTITY\n",
       "1212                                          TO PROPERTY\n",
       "419                                      VEHICLE - STOLEN\n",
       "713                                         GRAND LARCENY\n",
       "542                  AGRICULTURE & MRKTS LAW-UNCLASSIFIED\n",
       "703                               MISCELLANEOUS PENAL LAW\n",
       "1286                                           TO VEHICLE\n",
       "727                                            SEX CRIMES\n",
       "143     CRM AGNST CHLD (13 OR UNDER) (14-15 & SUSP 10 ...\n",
       "Name: Crime Description, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df['Crime Description'].sample(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n"
     ]
    }
   ],
   "source": [
    "combined_df['str_length'] = combined_df['Crime Description'].str.len()\n",
    "number_of_crimes = combined_df['Crime Description'].value_counts()\n",
    "list_of_crimes = combined_df['Crime Description'].unique().tolist()\n",
    "list_of_crimes_lower = [crime.lower() for crime in list_of_crimes]\n",
    "\n",
    "print(len(list_of_crimes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplify the Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Crime Description</th>\n",
       "      <th>City</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>THEFT PLAIN - PETTY ($950 &amp; UNDER)</td>\n",
       "      <td>LA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>BURGLARY</td>\n",
       "      <td>LA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>ANIMAL ABUSE / NEGLECT</td>\n",
       "      <td>CHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>DOCUMENT FORGERY / STOLEN FELONY</td>\n",
       "      <td>LA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>VEHICLE AND TRAFFIC LAWS</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>HARRASSMENT 2</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>THEFT OF IDENTITY</td>\n",
       "      <td>LA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>THEFT OF IDENTITY</td>\n",
       "      <td>LA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>THEFT OF IDENTITY</td>\n",
       "      <td>LA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>VANDALISM - FELONY ($400 &amp; OVER, ALL CHURCH VA...</td>\n",
       "      <td>LA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>VEHICLE AND TRAFFIC LAWS</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>HARRASSMENT 2</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>SEX,UNLAWFUL(INC MUTUAL CONSENT, PENETRATION W...</td>\n",
       "      <td>LA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>HARRASSMENT 2</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>TO PROPERTY</td>\n",
       "      <td>CHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>FIRST DEGREE MURDER</td>\n",
       "      <td>CHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352</th>\n",
       "      <td>$500 AND UNDER</td>\n",
       "      <td>CHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>VIOLATE ORDER OF PROTECTION</td>\n",
       "      <td>CHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>OVER $500</td>\n",
       "      <td>CHI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>HARRASSMENT 2</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Crime Description City\n",
       "230                  THEFT PLAIN - PETTY ($950 & UNDER)   LA\n",
       "431                                            BURGLARY   LA\n",
       "1400                             ANIMAL ABUSE / NEGLECT  CHI\n",
       "484                    DOCUMENT FORGERY / STOLEN FELONY   LA\n",
       "552                            VEHICLE AND TRAFFIC LAWS  NYC\n",
       "634                                       HARRASSMENT 2  NYC\n",
       "367                                   THEFT OF IDENTITY   LA\n",
       "478                                   THEFT OF IDENTITY   LA\n",
       "456                                   THEFT OF IDENTITY   LA\n",
       "429   VANDALISM - FELONY ($400 & OVER, ALL CHURCH VA...   LA\n",
       "631                            VEHICLE AND TRAFFIC LAWS  NYC\n",
       "591                                       HARRASSMENT 2  NYC\n",
       "288   SEX,UNLAWFUL(INC MUTUAL CONSENT, PENETRATION W...   LA\n",
       "656                                       HARRASSMENT 2  NYC\n",
       "1090                                        TO PROPERTY  CHI\n",
       "1231                                FIRST DEGREE MURDER  CHI\n",
       "1352                                     $500 AND UNDER  CHI\n",
       "1399                        VIOLATE ORDER OF PROTECTION  CHI\n",
       "1333                                          OVER $500  CHI\n",
       "592                                       HARRASSMENT 2  NYC"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df[['Crime Description', 'City']].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Lemmatized Text\n",
      "0                vehicle steal\n",
      "1        burglary from vehicle\n",
      "2                   bike steal\n",
      "3  shopliftinggrand theft over\n",
      "4            theft of identity\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import re\n",
    "try:\n",
    "    # Try to import the SpaCy model to check if it's installed\n",
    "    import en_core_web_sm\n",
    "except ImportError:\n",
    "    # If the model is not installed, download it using the subprocess module\n",
    "    print(\"Downloading the 'en_core_web_sm' model...\")\n",
    "    !python3.7 -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove digits and punctuation using a regular expression\n",
    "    text = re.sub(r'[\\d]', '', text)  # Remove digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation except underscores and spaces\n",
    "    return text.lower()\n",
    "\n",
    "def lemmatize(text):\n",
    "    # Apply spaCy nlp pipeline to pre-processed text\n",
    "    doc = nlp(text)\n",
    "    # Return lemmatized text, only including tokens that are alphabetic\n",
    "    return \" \".join([token.lemma_ for token in doc if token.is_alpha])\n",
    "\n",
    "# Assuming 'combined_df' is your DataFrame and 'Lemmatized Text' is the column to process\n",
    "# Step 1: Pre-process the text to remove digits and punctuation\n",
    "combined_df['Preprocessed Text'] = combined_df['Crime Description'].apply(preprocess_text)\n",
    "\n",
    "# Step 2: Apply lemmatization\n",
    "combined_df['Lemmatized Text'] = combined_df['Preprocessed Text'].apply(lemmatize)\n",
    "\n",
    "# Display the processed DataFrame\n",
    "print(combined_df[['Lemmatized Text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'stop_words' parameter of CountVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'somehow', 'fill', 'there', 'even', 'someone', 'itself', 'sometime', 'inc', 'system', 'amount', 'over', 'nothing', 'out', 'something', 'thence', 'along', 'latter', 'fifteen', 'thereupon', 'least', 'much', 'ord', 'bottom', 'else', 'if', 'fire', 'last', 'wherever', 'by', 'every', 'already', 'etc', 'together', 'eg', 'sometimes', 'eight', 'others', 'please', 'beside', 'anyhow', 'cannot', 'everywhere', 'where', 'could', 'hers', 'for', 'herein', 'move', 'own', 'be', 'they', 'formerly', 'another', 'three', 'via', 'hence', 'than', 'she', 'across', 'elsewhere', 'couldnt', 'thus', 'seems', 'mine', 'otherwise', 'former', 'nine', 'hereafter', 'has', 'the', 'at', 'thick', 'un', 'us', 'whenever', 'here', 'why', 'any', 'four', 'into', 'with', 'you', 'hereby', 'your', 'whereby', 'while', 'either', 'further', 'bill', 'or', 'front', 'towards', 'then', 'six', 'next', 'seemed', 're', 'moreover', 'within', 'do', 'perhaps', 'whereas', 'forty', 'down', 'as', 'eleven', 'which', 'interest', 'in', 'were', 'beforehand', 'very', 'co', 'ever', 'and', 'once', 'almost', 'ten', 'found', 'more', 'anywhere', 'rather', 'part', 'often', 'but', 'except', 'whereupon', 'up', 'thereby', 'always', 'afterwards', 'had', 'show', 'again', 'can', 'have', 'therefore', 'keep', 'ie', 'two', 'therein', 'that', 'less', 'when', 'ltd', 'nobody', 'an', 'first', 'same', 'hasnt', 'to', 'so', 'i', 'never', 'become', 'indeed', 'since', 'off', 'several', 'whither', 'are', 'describe', 'upon', 'due', 'well', 'too', 'nevertheless', 'seem', 'noone', 'only', 'below', 'might', 'find', 'thin', 'also', 'herself', 'been', 'none', 'their', 'whom', 'most', 'side', 'cry', 'get', 'alone', 'whose', 'meanwhile', 'whatever', 'him', 'whether', 'through', 'anyone', 'however', 'among', 'back', 'many', 'above', 'mill', 'yours', 'ourselves', 'top', 'full', 'of', 'though', 'empty', 'whence', 'throughout', 'yourselves', 'is', 'thereafter', 'serious', 'whole', 'myself', 'besides', 'under', 'what', 'sixty', 'should', 'per', 'amoungst', 'call', 'those', 'ours', 'without', 'am', 'no', 'take', 'twenty', 'before', 'such', 'sincere', 'everything', 'he', 'becoming', 'wherein', 'his', 'it', 'themselves', 'who', 'detail', 'namely', 'hereupon', 'everyone', 'seeming', 'now', 'not', 'until', 'because', 'third', 'go', 'me', 'anyway', 'on', 'cant', 'latterly', 'done', 'must', 'whereafter', 'a', 'whoever', 'amongst', 'neither', 'five', 'may', 'fifty', 'few', 'from', 'anything', 'con', 'nowhere', 'our', 'enough', 'behind', 'thru', 'see', 'name', 'give', 'them', 'one', 'during', 'becomes', 'nor', 'onto', 'this', 'would', 'its', 'how', 'my', 'these', 'other', 'being', 'was', 'made', 'beyond', 'somewhere', 'each', 'became', 'all', 'yet', 'some', 'around', 'both', 'mostly', 'about', 'himself', 'we', 'still', 'after', 'put', 'hundred', 'twelve', 'de', 'her', 'will', 'between', 'although', 'toward', 'yourself', 'against'}) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m max_df \u001b[38;5;129;01min\u001b[39;00m max_df_options:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m min_df \u001b[38;5;129;01min\u001b[39;00m min_df_options:\n\u001b[0;32m---> 41\u001b[0m         perplexity, top_words, topic_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_lda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_topics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombined_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_top_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m perplexity \u001b[38;5;241m<\u001b[39m lowest_perplexity:\n\u001b[1;32m     43\u001b[0m             lowest_perplexity \u001b[38;5;241m=\u001b[39m perplexity\n",
      "Cell \u001b[0;32mIn[19], line 11\u001b[0m, in \u001b[0;36mrun_lda\u001b[0;34m(n_topics, max_df, min_df, combined_df, top_topicwords)\u001b[0m\n\u001b[1;32m      9\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m ENGLISH_STOP_WORDS\u001b[38;5;241m.\u001b[39munion(custom_stop_words)\n\u001b[1;32m     10\u001b[0m cv \u001b[38;5;241m=\u001b[39m CountVectorizer(max_df\u001b[38;5;241m=\u001b[39mmax_df, min_df\u001b[38;5;241m=\u001b[39mmin_df, stop_words\u001b[38;5;241m=\u001b[39mstop_words)\n\u001b[0;32m---> 11\u001b[0m dtm \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLemmatized Text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m lda \u001b[38;5;241m=\u001b[39m LatentDirichletAllocation(n_components\u001b[38;5;241m=\u001b[39mn_topics, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     13\u001b[0m lda\u001b[38;5;241m.\u001b[39mfit(dtm)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:1467\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1462\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1463\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[1;32m   1464\u001b[0m )\n\u001b[1;32m   1466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[0;32m-> 1467\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[1;32m   1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:666\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    659\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \n\u001b[1;32m    661\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 666\u001b[0m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m     )\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m )\n",
      "\u001b[0;31mInvalidParameterError\u001b[0m: The 'stop_words' parameter of CountVectorizer must be a str among {'english'}, an instance of 'list' or None. Got frozenset({'somehow', 'fill', 'there', 'even', 'someone', 'itself', 'sometime', 'inc', 'system', 'amount', 'over', 'nothing', 'out', 'something', 'thence', 'along', 'latter', 'fifteen', 'thereupon', 'least', 'much', 'ord', 'bottom', 'else', 'if', 'fire', 'last', 'wherever', 'by', 'every', 'already', 'etc', 'together', 'eg', 'sometimes', 'eight', 'others', 'please', 'beside', 'anyhow', 'cannot', 'everywhere', 'where', 'could', 'hers', 'for', 'herein', 'move', 'own', 'be', 'they', 'formerly', 'another', 'three', 'via', 'hence', 'than', 'she', 'across', 'elsewhere', 'couldnt', 'thus', 'seems', 'mine', 'otherwise', 'former', 'nine', 'hereafter', 'has', 'the', 'at', 'thick', 'un', 'us', 'whenever', 'here', 'why', 'any', 'four', 'into', 'with', 'you', 'hereby', 'your', 'whereby', 'while', 'either', 'further', 'bill', 'or', 'front', 'towards', 'then', 'six', 'next', 'seemed', 're', 'moreover', 'within', 'do', 'perhaps', 'whereas', 'forty', 'down', 'as', 'eleven', 'which', 'interest', 'in', 'were', 'beforehand', 'very', 'co', 'ever', 'and', 'once', 'almost', 'ten', 'found', 'more', 'anywhere', 'rather', 'part', 'often', 'but', 'except', 'whereupon', 'up', 'thereby', 'always', 'afterwards', 'had', 'show', 'again', 'can', 'have', 'therefore', 'keep', 'ie', 'two', 'therein', 'that', 'less', 'when', 'ltd', 'nobody', 'an', 'first', 'same', 'hasnt', 'to', 'so', 'i', 'never', 'become', 'indeed', 'since', 'off', 'several', 'whither', 'are', 'describe', 'upon', 'due', 'well', 'too', 'nevertheless', 'seem', 'noone', 'only', 'below', 'might', 'find', 'thin', 'also', 'herself', 'been', 'none', 'their', 'whom', 'most', 'side', 'cry', 'get', 'alone', 'whose', 'meanwhile', 'whatever', 'him', 'whether', 'through', 'anyone', 'however', 'among', 'back', 'many', 'above', 'mill', 'yours', 'ourselves', 'top', 'full', 'of', 'though', 'empty', 'whence', 'throughout', 'yourselves', 'is', 'thereafter', 'serious', 'whole', 'myself', 'besides', 'under', 'what', 'sixty', 'should', 'per', 'amoungst', 'call', 'those', 'ours', 'without', 'am', 'no', 'take', 'twenty', 'before', 'such', 'sincere', 'everything', 'he', 'becoming', 'wherein', 'his', 'it', 'themselves', 'who', 'detail', 'namely', 'hereupon', 'everyone', 'seeming', 'now', 'not', 'until', 'because', 'third', 'go', 'me', 'anyway', 'on', 'cant', 'latterly', 'done', 'must', 'whereafter', 'a', 'whoever', 'amongst', 'neither', 'five', 'may', 'fifty', 'few', 'from', 'anything', 'con', 'nowhere', 'our', 'enough', 'behind', 'thru', 'see', 'name', 'give', 'them', 'one', 'during', 'becomes', 'nor', 'onto', 'this', 'would', 'its', 'how', 'my', 'these', 'other', 'being', 'was', 'made', 'beyond', 'somewhere', 'each', 'became', 'all', 'yet', 'some', 'around', 'both', 'mostly', 'about', 'himself', 'we', 'still', 'after', 'put', 'hundred', 'twelve', 'de', 'her', 'will', 'between', 'although', 'toward', 'yourself', 'against'}) instead."
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import pandas as pd  # Assuming you have a DataFrame 'combined_df'\n",
    "\n",
    "def run_lda(n_topics, max_df, min_df, combined_df, top_topicwords):\n",
    "\n",
    "    custom_stop_words = [\"ord\"]\n",
    "    stop_words = ENGLISH_STOP_WORDS.union(custom_stop_words)\n",
    "    cv = CountVectorizer(max_df=max_df, min_df=min_df, stop_words=stop_words)\n",
    "    dtm = cv.fit_transform(combined_df['Lemmatized Text'])\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    lda.fit(dtm)\n",
    "\n",
    "    top_words_per_topic = []\n",
    "    for index, topic in enumerate(lda.components_):\n",
    "        top_words = [cv.get_feature_names()[i] for i in topic.argsort()[-top_topicwords:]]\n",
    "        top_words_per_topic.append(top_words)\n",
    "        print(f'Top {top_topicwords} words for topic {index}: {top_words}')\n",
    "    \n",
    "    # Transform the DTM to get the topic results\n",
    "    topic_results = lda.transform(dtm)\n",
    "\n",
    "    # Return the perplexity and top words for each topic\n",
    "    return lda.perplexity(dtm), top_words_per_topic, topic_results\n",
    "\n",
    "# Initialize variables to keep track of the best model\n",
    "lowest_perplexity = float('inf')\n",
    "best_model_config = None\n",
    "best_top_words = None\n",
    "best_topic_results = None\n",
    "\n",
    "# Parameters to iterate over\n",
    "n_topics_options = [5, 7, 10]\n",
    "max_df_options = [0.7, 0.75, 0.8, 0.85]\n",
    "min_df_options = [2, 5]\n",
    "\n",
    "for n_topics in n_topics_options:\n",
    "    for max_df in max_df_options:\n",
    "        for min_df in min_df_options:\n",
    "            perplexity, top_words, topic_results = run_lda(n_topics, max_df, min_df, combined_df, best_top_words)\n",
    "            if perplexity < lowest_perplexity:\n",
    "                lowest_perplexity = perplexity\n",
    "                best_model_config = (n_topics, max_df, min_df)\n",
    "                best_top_words = top_words\n",
    "                best_topic_results = topic_results\n",
    "\n",
    "# Use the best model to append the dominant topic's top word to each document in the DataFrame\n",
    "dominant_topics = best_topic_results.argmax(axis=1)\n",
    "combined_df['Dominant Topic Word'] = [best_top_words[topic][0] for topic in dominant_topics]  # Using the first top word as representative\n",
    "\n",
    "print(f'Best Model Configuration: {best_model_config} with Lowest Perplexity: {lowest_perplexity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case Number</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Crime Code</th>\n",
       "      <th>Crime Description</th>\n",
       "      <th>Weapon</th>\n",
       "      <th>Victim Sex</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>City</th>\n",
       "      <th>str_length</th>\n",
       "      <th>Preprocessed Text</th>\n",
       "      <th>Lemmatized Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>12938222</td>\n",
       "      <td>2022-12-31T17:27:00.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1320</td>\n",
       "      <td>TO VEHICLE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.843082239</td>\n",
       "      <td>-87.631807329</td>\n",
       "      <td>CHI</td>\n",
       "      <td>10</td>\n",
       "      <td>to vehicle</td>\n",
       "      <td>to vehicle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275</th>\n",
       "      <td>12945997</td>\n",
       "      <td>2022-12-31T16:30:00.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1120</td>\n",
       "      <td>FORGERY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.000843647</td>\n",
       "      <td>-87.807064135</td>\n",
       "      <td>CHI</td>\n",
       "      <td>7</td>\n",
       "      <td>forgery</td>\n",
       "      <td>forgery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>12937863</td>\n",
       "      <td>2022-12-31T11:15:00.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1345</td>\n",
       "      <td>TO CITY OF CHICAGO PROPERTY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.754325322</td>\n",
       "      <td>-87.62710508</td>\n",
       "      <td>CHI</td>\n",
       "      <td>27</td>\n",
       "      <td>to city of chicago property</td>\n",
       "      <td>to city of chicago property</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366</th>\n",
       "      <td>12939476</td>\n",
       "      <td>2022-12-31T13:40:00.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0520</td>\n",
       "      <td>AGGRAVATED - KNIFE / CUTTING INSTRUMENT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.835655908</td>\n",
       "      <td>-87.647194113</td>\n",
       "      <td>CHI</td>\n",
       "      <td>39</td>\n",
       "      <td>aggravated  knife  cutting instrument</td>\n",
       "      <td>aggravate knife cut instrument</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>231220613</td>\n",
       "      <td>2023-10-02T00:00:00.000</td>\n",
       "      <td>2020-07-17T00:00:00.000</td>\n",
       "      <td>354</td>\n",
       "      <td>THEFT OF IDENTITY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>33.997</td>\n",
       "      <td>-118.2871</td>\n",
       "      <td>LA</td>\n",
       "      <td>17</td>\n",
       "      <td>theft of identity</td>\n",
       "      <td>theft of identity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Case Number                     Date                     Time Crime Code  \\\n",
       "1238    12938222  2022-12-31T17:27:00.000                      NaN       1320   \n",
       "1275    12945997  2022-12-31T16:30:00.000                      NaN       1120   \n",
       "1438    12937863  2022-12-31T11:15:00.000                      NaN       1345   \n",
       "1366    12939476  2022-12-31T13:40:00.000                      NaN       0520   \n",
       "248    231220613  2023-10-02T00:00:00.000  2020-07-17T00:00:00.000        354   \n",
       "\n",
       "                            Crime Description Weapon Victim Sex      Latitude  \\\n",
       "1238                               TO VEHICLE    NaN        NaN  41.843082239   \n",
       "1275                                  FORGERY    NaN        NaN  42.000843647   \n",
       "1438              TO CITY OF CHICAGO PROPERTY    NaN        NaN  41.754325322   \n",
       "1366  AGGRAVATED - KNIFE / CUTTING INSTRUMENT    NaN        NaN  41.835655908   \n",
       "248                         THEFT OF IDENTITY    NaN          F        33.997   \n",
       "\n",
       "          Longitude City  str_length                      Preprocessed Text  \\\n",
       "1238  -87.631807329  CHI          10                             to vehicle   \n",
       "1275  -87.807064135  CHI           7                                forgery   \n",
       "1438   -87.62710508  CHI          27            to city of chicago property   \n",
       "1366  -87.647194113  CHI          39  aggravated  knife  cutting instrument   \n",
       "248       -118.2871   LA          17                      theft of identity   \n",
       "\n",
       "                     Lemmatized Text  \n",
       "1238                      to vehicle  \n",
       "1275                         forgery  \n",
       "1438     to city of chicago property  \n",
       "1366  aggravate knife cut instrument  \n",
       "248                theft of identity  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '›' (U+203A) (2092999737.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 12\u001b[0;36m\u001b[0m\n\u001b[0;31m    ›combined_df[['Crime Description', 'topic', 'New Description']]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '›' (U+203A)\n"
     ]
    }
   ],
   "source": [
    "# new_descriptions = ['0': 'Traffic', '6':'Financial']\n",
    "combined_df['New Description'] = np.nan\n",
    "for index, row in combined_df.iterrows():\n",
    "    if row['topic'] == 0:\n",
    "        combined_df.loc[index, 'New Description'] = 'Vehicular'\n",
    "    elif row['topic'] == 6:\n",
    "        combined_df.loc[index, 'New Description'] = 'Financial'\n",
    "    elif row['topic'] == 8:\n",
    "        combined_df.loc[index, 'New Description'] = 'Larceny'\n",
    "    else:\n",
    "        combined_df.loc[index, 'New Description'] = 'Unassigned'\n",
    "›combined_df[['Crime Description', 'topic', 'New Description']]\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Extract feature names from the CountVectorizer\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Choose the number of top words to display for each topic\u001b[39;00m\n\u001b[1;32m     39\u001b[0m num_top_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cv' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_top_words_for_all_topics(lda_model, feature_names, num_top_words):\n",
    "    \"\"\"\n",
    "    Plots the top words for all topics in the LDA model.\n",
    "\n",
    "    Parameters:\n",
    "    - lda_model: The fitted LDA model.\n",
    "    - feature_names: The names of the features (words) from the CountVectorizer.\n",
    "    - num_top_words: The number of top words to include in each plot.\n",
    "    \"\"\"\n",
    "    # Number of topics\n",
    "    num_topics = lda_model.components_.shape[0]\n",
    "\n",
    "    # Create a figure to contain subplots for each topic.\n",
    "    fig, axes = plt.subplots(num_topics, 1, figsize=(10, 6 * num_topics), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for topic_idx, topic_word_weights in enumerate(lda_model.components_):\n",
    "        # Get the indices of the top words for this topic.\n",
    "        top_word_indices = topic_word_weights.argsort()[-num_top_words:][::-1]\n",
    "\n",
    "        # Get the top words and their weights.\n",
    "        top_words = [feature_names[i] for i in top_word_indices]\n",
    "        top_words_weights = topic_word_weights[top_word_indices]\n",
    "\n",
    "        # Plot for the current topic.\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_words, top_words_weights, color='lightblue')\n",
    "        ax.set_title(f'Topic {topic_idx + 1}', fontsize=14)\n",
    "        ax.invert_yaxis()  # Invert y-axis to have the highest weight on top.\n",
    "\n",
    "    plt.subplots_adjust(top=0.95, bottom=0.05, hspace=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Extract feature names from the CountVectorizer\n",
    "feature_names = cv.get_feature_names_out()\n",
    "\n",
    "# Choose the number of top words to display for each topic\n",
    "num_top_words = 10\n",
    "\n",
    "# Plot the top words for all topics\n",
    "plot_top_words_for_all_topics(lda, feature_names, num_top_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphing the crimes and their cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium import plugins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = combined_df \n",
    " \n",
    "# Convert Latitude and Longitude to float\n",
    "df['Latitude'] = df['Latitude'].astype(float)\n",
    "df['Longitude'] = df['Longitude'].astype(float)\n",
    " \n",
    "# Create a map centered around the mean of latitude and longitude\n",
    "map1 = folium.Map(location=[df['Latitude'].mean(), df['Longitude'].mean()], zoom_start=10)\n",
    " \n",
    "# Add markers for each crime location\n",
    "for index, row in df.iterrows():\n",
    "    # Check for NaN values\n",
    "    if not pd.isnull(row['Latitude']) and not pd.isnull(row['Longitude']):\n",
    "        folium.Marker(location=[row['Latitude'], row['Longitude']], popup=row['Case Number']).add_to(m)\n",
    "\n",
    "# Save the map\n",
    "filename = 'crime_map.html'  # Specify a full file path\n",
    "#map1.save(filename)\n",
    "map1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_lon = -118.2426\n",
    "la_lat = 34.0549\n",
    "#Create a second map with the same LA coordinates\n",
    "la_map2 = folium.Map(location = [la_lat, la_lon], zoom_start = 9)\n",
    "\n",
    "#Instantiate a mark cluster object for the incidents in the dataframe\n",
    "incidents2 = plugins.MarkerCluster().add_to(la_map2)\n",
    "\n",
    "df = df.dropna(subset=['Longitude','Latitude'])\n",
    "\n",
    "#print(type(lat))\n",
    "#Loop through the dataframe and add each data point to the mark cluster\n",
    "for lat, lng, in zip(df.Latitude, df.Longitude):\n",
    "    \n",
    "    folium.Marker(\n",
    "        location=[lat, lng],\n",
    "        icon=None,\n",
    "    ).add_to(incidents2)\n",
    "\n",
    "#Display map\n",
    "la_map2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e534e48711db4d1e1c48977d0d14ff85b1f16d41bcc4fdfd88268a329b3c9d66"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
