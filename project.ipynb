{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NYC API Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "503 Server Error: Service Temporarily Unavailable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lr/kh19kn_12gn2gvxzgwb23wjh0000gn/T/ipykernel_56068/682226673.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# First 2000 results, returned as JSON from API / converted to Python list of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# dictionaries by sodapy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mnyc_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnyc_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"5uac-w243\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnyc_limit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Convert to pandas DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sodapy/socrata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, dataset_identifier, content_type, **kwargs)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         response = self._perform_request(\n\u001b[0;32m--> 413\u001b[0;31m             \u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m         )\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sodapy/socrata.py\u001b[0m in \u001b[0;36m_perform_request\u001b[0;34m(self, request_type, resource, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;31m# handle errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m202\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;31m# when responses have no content body (ie. delete, set_permission),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sodapy/utils.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmore_info\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmore_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mhttp_error_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\".\\n\\t{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmore_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 503 Server Error: Service Temporarily Unavailable"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sodapy import Socrata\n",
    "\n",
    "# Number of Records limit\n",
    "nyc_limit = 500\n",
    "la_limit = 500\n",
    "chi_limit = 500\n",
    "\n",
    "# Connect to NYC API\n",
    "NYCAppToken = '3u5hcZ6WwKere5Mb5nm5S9mT2'\n",
    "nyc_client = Socrata(\"data.cityofnewyork.us\",\n",
    "                 NYCAppToken,\n",
    "                 username=\"Cameron.Suddreth@du.edu\",\n",
    "                 password=\"COMP4447groupproject\")\n",
    "\n",
    "# First 2000 results, returned as JSON from API / converted to Python list of\n",
    "# dictionaries by sodapy.\n",
    "nyc_results = nyc_client.get(\"5uac-w243\", limit=nyc_limit)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "nyc_df = pd.DataFrame.from_records(nyc_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LA API Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example authenticated client (needed for non-public datasets):\n",
    "LA_AppToken = 'mEU8HkgWCvfkWLHKGxfiUFecc'\n",
    "la_client = Socrata(\"data.lacity.org\",\n",
    "                 LA_AppToken,\n",
    "                 username=\"Cameron.Suddreth@du.edu\",\n",
    "                 password=\"COMP4447groupproject\")\n",
    "\n",
    "\n",
    "la_results = la_client.get(\"2nrs-mtv8\", limit=la_limit)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "la_df = pd.DataFrame.from_records(la_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chicago API Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHI_AppToken = '6rxQVr5BfXAbUUccKTodxYVdj'\n",
    "chi_client = Socrata(\"data.cityofchicago.org\",\n",
    "                    CHI_AppToken,\n",
    "                    username=\"Cameron.Suddreth@du.edu\",\n",
    "                    password=\"COMP4447groupproject\")\n",
    "chi_results = chi_client.get(\"9hwr-2zxp\", limit=chi_limit)\n",
    "chi_df = pd.DataFrame.from_records(chi_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we have established three different API connections. We have established an API connection with each of the three largest cities in the United States: New York, NY; Los Angeles, CA; and Chicago, IL. We are also able to change the number of records that we are pulling in for analysis as each of the databases combined would result in close to one million records!\n",
    "\n",
    "With each connection, we have created a separate dataframe which allowed us to easily pull in all the records from each city's database. As we now have each of the cities with their own dataframe, we will begin to merge the dataframes together to allow us to look at the data amongst the cities together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell compares the number of columns within each dataframe. This was the initial step for us to begin merging the dataset together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_columns = nyc_df.columns\n",
    "la_columns = la_df.columns\n",
    "chi_columns = chi_df.columns\n",
    "\n",
    "print(f'Number of NYC Columns: {len(nyc_columns)}')\n",
    "print(f'Number of LA Columns: {len(la_columns)}')\n",
    "print(f'Number of CHI Columns: {len(chi_columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reviewed the column names within the API documentation and determined what information we wanted from each city and then reduced the size of each dataframe to contain only those columns which we wanted to analyze. We then renamed each column so to allow for easier analysis and merging.\n",
    "\n",
    "Before we create the new column headers, we must also resolve any NAN values else we will receive an error. We have imported the Numpy module to fill in the Weapon user for each NYC and Chicago; the time of the crime in Chicago; and the victim sex for Chicago as this data was unavailable in each respective city's database.\n",
    "\n",
    "We have also created a new column in each city's original dataframe to help identify which city each record belongs to after we have merged the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "nyc_df['Weapon'] = np.nan\n",
    "chi_df['Weapon'] = np.nan\n",
    "chi_df['Time'] = np.nan\n",
    "chi_df['Victim Sex'] = np.nan\n",
    "nyc_df['City'] = 'NYC'\n",
    "la_df['City'] = \"LA\"\n",
    "chi_df['City'] = 'CHI'\n",
    "\n",
    "la_df = la_df[['dr_no', 'date_rptd', 'date_occ', 'crm_cd', 'crm_cd_desc', \n",
    "'weapon_desc', 'vict_sex', 'lat', 'lon', 'City']]\n",
    "nyc_df = nyc_df[['cmplnt_num', 'cmplnt_fr_dt', 'cmplnt_fr_tm','ky_cd', 'ofns_desc', \n",
    "'Weapon', 'vic_sex', 'latitude','longitude', 'City']]\n",
    "chi_df = chi_df[['id', 'date','Time', 'iucr','description','Weapon', \n",
    "'Victim Sex','latitude','longitude', 'City']]\n",
    "generic_columns = ['Case Number', 'Date', 'Time', 'Crime Code', 'Crime Description','Weapon', \n",
    "'Victim Sex', 'Latitude', 'Longitude', 'City']\n",
    "\n",
    "# Rename the columns\n",
    "la_df.columns = generic_columns\n",
    "nyc_df.columns = generic_columns\n",
    "chi_df.columns = generic_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By renaming the columns of the dataframe above, it simplified the merging process, so we did not have to specify what column in each dataframe to merge based on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([la_df, nyc_df, chi_df],ignore_index=True)\n",
    "print(len(combined_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['Crime Description'].sample(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['str_length'] = combined_df['Crime Description'].str.len()\n",
    "number_of_crimes = combined_df['Crime Description'].value_counts()\n",
    "list_of_crimes = combined_df['Crime Description'].unique().tolist()\n",
    "list_of_crimes_lower = [crime.lower() for crime in list_of_crimes]\n",
    "\n",
    "print(len(list_of_crimes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplify the Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[['Crime Description', 'City']].sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "try:\n",
    "    # Try to import the SpaCy model to check if it's installed\n",
    "    import en_core_web_sm\n",
    "except ImportError:\n",
    "    # If the model is not installed, download it using the subprocess module\n",
    "    print(\"Downloading the 'en_core_web_sm' model...\")\n",
    "    !python3.7 -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove digits and punctuation using a regular expression\n",
    "    text = re.sub(r'[\\d]', '', text)  # Remove digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation except underscores and spaces\n",
    "    return text.lower()\n",
    "\n",
    "def lemmatize(text):\n",
    "    # Apply spaCy nlp pipeline to pre-processed text\n",
    "    doc = nlp(text)\n",
    "    # Return lemmatized text, only including tokens that are alphabetic\n",
    "    return \" \".join([token.lemma_ for token in doc if token.is_alpha])\n",
    "\n",
    "# Assuming 'combined_df' is your DataFrame and 'Lemmatized Text' is the column to process\n",
    "# Step 1: Pre-process the text to remove digits and punctuation\n",
    "combined_df['Preprocessed Text'] = combined_df['Crime Description'].apply(preprocess_text)\n",
    "\n",
    "# Step 2: Apply lemmatization\n",
    "combined_df['Lemmatized Text'] = combined_df['Preprocessed Text'].apply(lemmatize)\n",
    "\n",
    "# Display the processed DataFrame\n",
    "print(combined_df[['Lemmatized Text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import pandas as pd  # Assuming you have a DataFrame 'combined_df'\n",
    "\n",
    "def run_lda(n_topics, max_df, min_df, combined_df, top_topicwords):\n",
    "\n",
    "    custom_stop_words = [\"ord\"]\n",
    "    stop_words = ENGLISH_STOP_WORDS.union(custom_stop_words)\n",
    "    cv = CountVectorizer(max_df=max_df, min_df=min_df, stop_words=stop_words)\n",
    "    dtm = cv.fit_transform(combined_df['Lemmatized Text'])\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    lda.fit(dtm)\n",
    "\n",
    "    top_words_per_topic = []\n",
    "    for index, topic in enumerate(lda.components_):\n",
    "        top_words = [cv.get_feature_names()[i] for i in topic.argsort()[-top_topicwords:]]\n",
    "        top_words_per_topic.append(top_words)\n",
    "        print(f'Top {top_topicwords} words for topic {index}: {top_words}')\n",
    "    \n",
    "    # Transform the DTM to get the topic results\n",
    "    topic_results = lda.transform(dtm)\n",
    "\n",
    "    # Return the perplexity and top words for each topic\n",
    "    return lda.perplexity(dtm), top_words_per_topic, topic_results\n",
    "\n",
    "# Initialize variables to keep track of the best model\n",
    "lowest_perplexity = float('inf')\n",
    "best_model_config = None\n",
    "best_top_words = None\n",
    "best_topic_results = None\n",
    "\n",
    "# Parameters to iterate over\n",
    "n_topics_options = [5, 7, 10]\n",
    "max_df_options = [0.7, 0.75, 0.8, 0.85]\n",
    "min_df_options = [2, 5]\n",
    "\n",
    "for n_topics in n_topics_options:\n",
    "    for max_df in max_df_options:\n",
    "        for min_df in min_df_options:\n",
    "            perplexity, top_words, topic_results = run_lda(n_topics, max_df, min_df, combined_df, top_topicwords)\n",
    "            if perplexity < lowest_perplexity:\n",
    "                lowest_perplexity = perplexity\n",
    "                best_model_config = (n_topics, max_df, min_df)\n",
    "                best_top_words = top_words\n",
    "                best_topic_results = topic_results\n",
    "\n",
    "# Use the best model to append the dominant topic's top word to each document in the DataFrame\n",
    "dominant_topics = best_topic_results.argmax(axis=1)\n",
    "combined_df['Dominant Topic Word'] = [best_top_words[topic][0] for topic in dominant_topics]  # Using the first top word as representative\n",
    "\n",
    "print(f'Best Model Configuration: {best_model_config} with Lowest Perplexity: {lowest_perplexity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_descriptions = ['0': 'Traffic', '6':'Financial']\n",
    "combined_df['New Description'] = np.nan\n",
    "for index, row in combined_df.iterrows():\n",
    "    if row['topic'] == 0:\n",
    "        combined_df.loc[index, 'New Description'] = 'Vehicular'\n",
    "    elif row['topic'] == 6:\n",
    "        combined_df.loc[index, 'New Description'] = 'Financial'\n",
    "    elif row['topic'] == 8:\n",
    "        combined_df.loc[index, 'New Description'] = 'Larceny'\n",
    "    else:\n",
    "        combined_df.loc[index, 'New Description'] = 'Unassigned'\n",
    "›combined_df[['Crime Description', 'topic', 'New Description']]\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_top_words_for_all_topics(lda_model, feature_names, num_top_words):\n",
    "    \"\"\"\n",
    "    Plots the top words for all topics in the LDA model.\n",
    "\n",
    "    Parameters:\n",
    "    - lda_model: The fitted LDA model.\n",
    "    - feature_names: The names of the features (words) from the CountVectorizer.\n",
    "    - num_top_words: The number of top words to include in each plot.\n",
    "    \"\"\"\n",
    "    # Number of topics\n",
    "    num_topics = lda_model.components_.shape[0]\n",
    "\n",
    "    # Create a figure to contain subplots for each topic.\n",
    "    fig, axes = plt.subplots(num_topics, 1, figsize=(10, 6 * num_topics), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for topic_idx, topic_word_weights in enumerate(lda_model.components_):\n",
    "        # Get the indices of the top words for this topic.\n",
    "        top_word_indices = topic_word_weights.argsort()[-num_top_words:][::-1]\n",
    "\n",
    "        # Get the top words and their weights.\n",
    "        top_words = [feature_names[i] for i in top_word_indices]\n",
    "        top_words_weights = topic_word_weights[top_word_indices]\n",
    "\n",
    "        # Plot for the current topic.\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_words, top_words_weights, color='lightblue')\n",
    "        ax.set_title(f'Topic {topic_idx + 1}', fontsize=14)\n",
    "        ax.invert_yaxis()  # Invert y-axis to have the highest weight on top.\n",
    "\n",
    "    plt.subplots_adjust(top=0.95, bottom=0.05, hspace=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Extract feature names from the CountVectorizer\n",
    "feature_names = cv.get_feature_names_out()\n",
    "\n",
    "# Choose the number of top words to display for each topic\n",
    "num_top_words = 10\n",
    "\n",
    "# Plot the top words for all topics\n",
    "plot_top_words_for_all_topics(lda, feature_names, num_top_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphing the crimes and their cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    " \n",
    "df = combined_df\n",
    " \n",
    " \n",
    "# Convert Latitude and Longitude to float\n",
    "df['Latitude'] = df['Latitude'].astype(float)\n",
    "df['Longitude'] = df['Longitude'].astype(float)\n",
    " \n",
    "# Create a map centered around the mean of latitude and longitude\n",
    "m = folium.Map(location=[df['Latitude'].mean(), df['Longitude'].mean()], zoom_start=10)\n",
    " \n",
    "# Add markers for each crime location\n",
    "for index, row in df.iterrows():\n",
    "    # Check for NaN values\n",
    "    if not pd.isnull(row['Latitude']) and not pd.isnull(row['Longitude']):\n",
    "        folium.Marker(location=[row['Latitude'], row['Longitude']], popup=row['Case Number']).add_to(m)\n",
    " \n",
    "# Save the map\n",
    "filename = 'crime_map.html'  # Specify a full file path\n",
    "m.save(filename)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e534e48711db4d1e1c48977d0d14ff85b1f16d41bcc4fdfd88268a329b3c9d66"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
